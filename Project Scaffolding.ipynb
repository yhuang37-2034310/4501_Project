{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "32f8ca24",
   "metadata": {},
   "source": [
    "# Understanding Hired Rides in NYC\n",
    "\n",
    "_[Project prompt](https://docs.google.com/document/d/1VERPjEZcC1XSs4-02aM-DbkNr_yaJVbFjLJxaYQswqA/edit#)_\n",
    "\n",
    "_This scaffolding notebook may be used to help setup your final project. It's **totally optional** whether you make use of this or not._\n",
    "\n",
    "_If you do use this notebook, everything provided is optional as well - you may remove or add prose and code as you wish._\n",
    "\n",
    "_Anything in italics (prose) or comments (in code) is meant to provide you with guidance. **Remove the italic lines and provided comments** before submitting the project, if you choose to use this scaffolding. We don't need the guidance when grading._\n",
    "\n",
    "_**All code below should be consider \"pseudo-code\" - not functional by itself, and only a suggestion at the approach.**_"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2f75fd94",
   "metadata": {},
   "source": [
    "## Project Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "10434bfd-2214-4f09-bd1e-5d342f60c17c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: geopandas in /opt/anaconda3/lib/python3.12/site-packages (1.0.1)\n",
      "Requirement already satisfied: numpy>=1.22 in /opt/anaconda3/lib/python3.12/site-packages (from geopandas) (1.26.4)\n",
      "Requirement already satisfied: pyogrio>=0.7.2 in /opt/anaconda3/lib/python3.12/site-packages (from geopandas) (0.10.0)\n",
      "Requirement already satisfied: packaging in /opt/anaconda3/lib/python3.12/site-packages (from geopandas) (23.2)\n",
      "Requirement already satisfied: pandas>=1.4.0 in /opt/anaconda3/lib/python3.12/site-packages (from geopandas) (2.2.2)\n",
      "Requirement already satisfied: pyproj>=3.3.0 in /opt/anaconda3/lib/python3.12/site-packages (from geopandas) (3.6.1)\n",
      "Requirement already satisfied: shapely>=2.0.0 in /opt/anaconda3/lib/python3.12/site-packages (from geopandas) (2.0.5)\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in /opt/anaconda3/lib/python3.12/site-packages (from pandas>=1.4.0->geopandas) (2.9.0.post0)\n",
      "Requirement already satisfied: pytz>=2020.1 in /opt/anaconda3/lib/python3.12/site-packages (from pandas>=1.4.0->geopandas) (2024.1)\n",
      "Requirement already satisfied: tzdata>=2022.7 in /opt/anaconda3/lib/python3.12/site-packages (from pandas>=1.4.0->geopandas) (2023.3)\n",
      "Requirement already satisfied: certifi in /opt/anaconda3/lib/python3.12/site-packages (from pyogrio>=0.7.2->geopandas) (2024.8.30)\n",
      "Requirement already satisfied: six>=1.5 in /opt/anaconda3/lib/python3.12/site-packages (from python-dateutil>=2.8.2->pandas>=1.4.0->geopandas) (1.16.0)\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install geopandas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "66dcde05",
   "metadata": {},
   "outputs": [],
   "source": [
    "# all import statements needed for the project, for example:\n",
    "\n",
    "import os\n",
    "\n",
    "import bs4\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "import requests\n",
    "import sqlalchemy as db\n",
    "import re\n",
    "import geopandas as gpd\n",
    "import math\n",
    "import glob\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "3f1242c4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# any constants you might need; some have been added for you, and \n",
    "# some you need to fill in\n",
    "\n",
    "TLC_URL = \"https://www1.nyc.gov/site/tlc/about/tlc-trip-record-data.page\"\n",
    "\n",
    "PARQUET_FILES = \"parquet_files\"\n",
    "TAXI_ZONES_DIR = \"taxi_zones\"\n",
    "TAXI_ZONES_SHAPEFILE = f\"{TAXI_ZONES_DIR}/taxi_zones.shp\"\n",
    "WEATHER_CSV_DIR = \"weather_data\"\n",
    "\n",
    "#CRS = 4326  # coordinate reference system\n",
    "\n",
    "# (lat, lon)\n",
    "#NEW_YORK_BOX_COORDS = ((40.560445, -74.242330), (40.908524, -73.717047))\n",
    "LGA_BOX_COORDS = ((40.763589, -73.891745), (40.778865, -73.854838))\n",
    "JFK_BOX_COORDS = ((40.639263, -73.795642), (40.651376, -73.766264))\n",
    "EWR_BOX_COORDS = ((40.686794, -74.194028), (40.699680, -74.165205))\n",
    "\n",
    "DATABASE_URL = \"sqlite:///project.db\"\n",
    "DATABASE_SCHEMA_FILE = \"schema.sql\"\n",
    "QUERY_DIRECTORY = \"queries\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "d6601633",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Make sure the QUERY_DIRECTORY exists\n",
    "try:\n",
    "    os.mkdir(QUERY_DIRECTORY)\n",
    "except Exception as e:\n",
    "    if e.errno == 17:\n",
    "        # the directory already exists\n",
    "        pass\n",
    "    else:\n",
    "        raise"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "26ad10ea",
   "metadata": {},
   "source": [
    "## Part 1: Data Preprocessing"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8be67f9a-979b-429c-805e-95b05b023728",
   "metadata": {},
   "source": [
    "### Download All data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "4ae91409-8d6a-4fb9-a26a-70a1487ed0e5",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_all_urls_from_tlc_page():\n",
    "    response = requests.get(TLC_URL)\n",
    "    html = response.content\n",
    "    return html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "1f70e242-38bb-431f-abb3-b0405beb1b56",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Identifies all of the yellow and fhvhv parquet files for years 2020 - 2024\n",
    "pattern = re.compile(r\".*(yellow|fhvhv).*(2020|2021|2022|2023|2024)-\\d{2}\\.parquet\")\n",
    "\n",
    "def filter_parquet_urls():\n",
    "    html = get_all_urls_from_tlc_page()\n",
    "    soup = bs4.BeautifulSoup(html, \"html.parser\")\n",
    "    urls = soup.find_all(\"a\", href=pattern)\n",
    "    parquet_urls = [link[\"href\"].strip() for link in urls]\n",
    "    return parquet_urls"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9c8e93ae-18cb-4edf-aaec-bec22b88b000",
   "metadata": {},
   "outputs": [],
   "source": [
    "folder_name = \"parquet_files\"\n",
    "\n",
    "# Check if the folder exists\n",
    "if not os.path.exists(folder_name):\n",
    "    os.mkdir(folder_name)\n",
    "    print(f\"Folder '{folder_name}' created successfully!\")\n",
    "else:\n",
    "    print(f\"Folder '{folder_name}' already exists.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b99805f5-a06a-4324-8c97-692e7a7f9ea3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# parses the filename from the link and then downloads the files one by one\n",
    "def download_parquet_files():\n",
    "    for link in filter_parquet_urls():\n",
    "        filename = link.split(\"/\")[-1]\n",
    "        r = requests.get(link)\n",
    "        with open(f\"parquet_files/{filename}\", \"wb\") as f:\n",
    "            f.write(r.content)\n",
    "\n",
    "#run the first time to download data\n",
    "#download_parquet_files()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d0d53e24",
   "metadata": {},
   "source": [
    "### Load Taxi Zones & Parquet Files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "58708809",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Reads the shape file\n",
    "def load_taxi_zones(shapefile):\n",
    "    taxi_zones = gpd.read_file(shapefile)\n",
    "    return taxi_zones"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "f8171ce1-7305-4091-b7ba-72f6a7799987",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   OBJECTID  Shape_Leng  Shape_Area                     zone  LocationID  \\\n",
      "0         1    0.116357    0.000782           Newark Airport           1   \n",
      "1         2    0.433470    0.004866              Jamaica Bay           2   \n",
      "2         3    0.084341    0.000314  Allerton/Pelham Gardens           3   \n",
      "3         4    0.043567    0.000112            Alphabet City           4   \n",
      "4         5    0.092146    0.000498            Arden Heights           5   \n",
      "\n",
      "         borough                                           geometry  \n",
      "0            EWR  POLYGON ((933100.918 192536.086, 933091.011 19...  \n",
      "1         Queens  MULTIPOLYGON (((1033269.244 172126.008, 103343...  \n",
      "2          Bronx  POLYGON ((1026308.77 256767.698, 1026495.593 2...  \n",
      "3      Manhattan  POLYGON ((992073.467 203714.076, 992068.667 20...  \n",
      "4  Staten Island  POLYGON ((935843.31 144283.336, 936046.565 144...  \n"
     ]
    }
   ],
   "source": [
    "gdf_taxi_zones = load_taxi_zones(TAXI_ZONES_SHAPEFILE)\n",
    "print(gdf_taxi_zones.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "64f2431c-4839-469e-a375-327a698e4356",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>OBJECTID</th>\n",
       "      <th>Shape_Leng</th>\n",
       "      <th>Shape_Area</th>\n",
       "      <th>zone</th>\n",
       "      <th>LocationID</th>\n",
       "      <th>borough</th>\n",
       "      <th>geometry</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>0.116357</td>\n",
       "      <td>0.000782</td>\n",
       "      <td>Newark Airport</td>\n",
       "      <td>1</td>\n",
       "      <td>EWR</td>\n",
       "      <td>POLYGON ((-74.18445 40.695, -74.18449 40.6951,...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2</td>\n",
       "      <td>0.433470</td>\n",
       "      <td>0.004866</td>\n",
       "      <td>Jamaica Bay</td>\n",
       "      <td>2</td>\n",
       "      <td>Queens</td>\n",
       "      <td>MULTIPOLYGON (((-73.82338 40.63899, -73.82277 ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3</td>\n",
       "      <td>0.084341</td>\n",
       "      <td>0.000314</td>\n",
       "      <td>Allerton/Pelham Gardens</td>\n",
       "      <td>3</td>\n",
       "      <td>Bronx</td>\n",
       "      <td>POLYGON ((-73.84793 40.87134, -73.84725 40.870...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>4</td>\n",
       "      <td>0.043567</td>\n",
       "      <td>0.000112</td>\n",
       "      <td>Alphabet City</td>\n",
       "      <td>4</td>\n",
       "      <td>Manhattan</td>\n",
       "      <td>POLYGON ((-73.97177 40.72582, -73.97179 40.725...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>5</td>\n",
       "      <td>0.092146</td>\n",
       "      <td>0.000498</td>\n",
       "      <td>Arden Heights</td>\n",
       "      <td>5</td>\n",
       "      <td>Staten Island</td>\n",
       "      <td>POLYGON ((-74.17422 40.56257, -74.17349 40.562...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>258</th>\n",
       "      <td>259</td>\n",
       "      <td>0.126750</td>\n",
       "      <td>0.000395</td>\n",
       "      <td>Woodlawn/Wakefield</td>\n",
       "      <td>259</td>\n",
       "      <td>Bronx</td>\n",
       "      <td>POLYGON ((-73.85107 40.91037, -73.85207 40.909...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>259</th>\n",
       "      <td>260</td>\n",
       "      <td>0.133514</td>\n",
       "      <td>0.000422</td>\n",
       "      <td>Woodside</td>\n",
       "      <td>260</td>\n",
       "      <td>Queens</td>\n",
       "      <td>POLYGON ((-73.90175 40.76078, -73.90147 40.759...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>260</th>\n",
       "      <td>261</td>\n",
       "      <td>0.027120</td>\n",
       "      <td>0.000034</td>\n",
       "      <td>World Trade Center</td>\n",
       "      <td>261</td>\n",
       "      <td>Manhattan</td>\n",
       "      <td>POLYGON ((-74.01333 40.70503, -74.01327 40.704...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>261</th>\n",
       "      <td>262</td>\n",
       "      <td>0.049064</td>\n",
       "      <td>0.000122</td>\n",
       "      <td>Yorkville East</td>\n",
       "      <td>262</td>\n",
       "      <td>Manhattan</td>\n",
       "      <td>MULTIPOLYGON (((-73.94383 40.78286, -73.94376 ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>262</th>\n",
       "      <td>263</td>\n",
       "      <td>0.037017</td>\n",
       "      <td>0.000066</td>\n",
       "      <td>Yorkville West</td>\n",
       "      <td>263</td>\n",
       "      <td>Manhattan</td>\n",
       "      <td>POLYGON ((-73.95219 40.77302, -73.95269 40.772...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>263 rows × 7 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "     OBJECTID  Shape_Leng  Shape_Area                     zone  LocationID  \\\n",
       "0           1    0.116357    0.000782           Newark Airport           1   \n",
       "1           2    0.433470    0.004866              Jamaica Bay           2   \n",
       "2           3    0.084341    0.000314  Allerton/Pelham Gardens           3   \n",
       "3           4    0.043567    0.000112            Alphabet City           4   \n",
       "4           5    0.092146    0.000498            Arden Heights           5   \n",
       "..        ...         ...         ...                      ...         ...   \n",
       "258       259    0.126750    0.000395       Woodlawn/Wakefield         259   \n",
       "259       260    0.133514    0.000422                 Woodside         260   \n",
       "260       261    0.027120    0.000034       World Trade Center         261   \n",
       "261       262    0.049064    0.000122           Yorkville East         262   \n",
       "262       263    0.037017    0.000066           Yorkville West         263   \n",
       "\n",
       "           borough                                           geometry  \n",
       "0              EWR  POLYGON ((-74.18445 40.695, -74.18449 40.6951,...  \n",
       "1           Queens  MULTIPOLYGON (((-73.82338 40.63899, -73.82277 ...  \n",
       "2            Bronx  POLYGON ((-73.84793 40.87134, -73.84725 40.870...  \n",
       "3        Manhattan  POLYGON ((-73.97177 40.72582, -73.97179 40.725...  \n",
       "4    Staten Island  POLYGON ((-74.17422 40.56257, -74.17349 40.562...  \n",
       "..             ...                                                ...  \n",
       "258          Bronx  POLYGON ((-73.85107 40.91037, -73.85207 40.909...  \n",
       "259         Queens  POLYGON ((-73.90175 40.76078, -73.90147 40.759...  \n",
       "260      Manhattan  POLYGON ((-74.01333 40.70503, -74.01327 40.704...  \n",
       "261      Manhattan  MULTIPOLYGON (((-73.94383 40.78286, -73.94376 ...  \n",
       "262      Manhattan  POLYGON ((-73.95219 40.77302, -73.95269 40.772...  \n",
       "\n",
       "[263 rows x 7 columns]"
      ]
     },
     "execution_count": 63,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# converts taxi zone geometry coordinates to the appropriate coordinate system  \n",
    "gdf_taxi_zones = gdf_taxi_zones.to_crs(epsg=4326)\n",
    "gdf_taxi_zones"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "4f6da777-79a3-48f2-8871-0d2d9c3aa0ea",
   "metadata": {},
   "outputs": [],
   "source": [
    "# load parquet file into a pandas DataFrame\n",
    "def load_parquet_file(file_path):\n",
    "    df = pd.read_parquet(file_path)\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "45a6df80-fe06-41ab-97bc-800f117a9c5e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   VendorID tpep_pickup_datetime tpep_dropoff_datetime  passenger_count  \\\n",
      "0         2  2023-01-01 00:32:10   2023-01-01 00:40:36              1.0   \n",
      "1         2  2023-01-01 00:55:08   2023-01-01 01:01:27              1.0   \n",
      "2         2  2023-01-01 00:25:04   2023-01-01 00:37:49              1.0   \n",
      "3         1  2023-01-01 00:03:48   2023-01-01 00:13:25              0.0   \n",
      "4         2  2023-01-01 00:10:29   2023-01-01 00:21:19              1.0   \n",
      "\n",
      "   trip_distance  RatecodeID store_and_fwd_flag  PULocationID  DOLocationID  \\\n",
      "0           0.97         1.0                  N           161           141   \n",
      "1           1.10         1.0                  N            43           237   \n",
      "2           2.51         1.0                  N            48           238   \n",
      "3           1.90         1.0                  N           138             7   \n",
      "4           1.43         1.0                  N           107            79   \n",
      "\n",
      "   payment_type  fare_amount  extra  mta_tax  tip_amount  tolls_amount  \\\n",
      "0             2          9.3   1.00      0.5        0.00           0.0   \n",
      "1             1          7.9   1.00      0.5        4.00           0.0   \n",
      "2             1         14.9   1.00      0.5       15.00           0.0   \n",
      "3             1         12.1   7.25      0.5        0.00           0.0   \n",
      "4             1         11.4   1.00      0.5        3.28           0.0   \n",
      "\n",
      "   improvement_surcharge  total_amount  congestion_surcharge  airport_fee  \n",
      "0                    1.0         14.30                   2.5         0.00  \n",
      "1                    1.0         16.90                   2.5         0.00  \n",
      "2                    1.0         34.90                   2.5         0.00  \n",
      "3                    1.0         20.85                   0.0         1.25  \n",
      "4                    1.0         19.68                   2.5         0.00  \n"
     ]
    }
   ],
   "source": [
    "# load a random yellow taxi trip parquet file to check if the function works correctly for testing purposes\n",
    "example = os.path.join(PARQUET_FILES, \"yellow_tripdata_2023-01.parquet\")\n",
    "example_df = load_parquet_file(example)\n",
    "\n",
    "# preview the data\n",
    "print(example_df.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "09fc14f2-021e-4222-a592-b77e25aa3882",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  hvfhs_license_num dispatching_base_num originating_base_num  \\\n",
      "0            HV0003               B03404               B03404   \n",
      "1            HV0003               B03404               B03404   \n",
      "2            HV0003               B03404               B03404   \n",
      "3            HV0003               B03404               B03404   \n",
      "4            HV0003               B03404               B03404   \n",
      "\n",
      "     request_datetime   on_scene_datetime     pickup_datetime  \\\n",
      "0 2023-01-01 00:18:06 2023-01-01 00:19:24 2023-01-01 00:19:38   \n",
      "1 2023-01-01 00:48:42 2023-01-01 00:56:20 2023-01-01 00:58:39   \n",
      "2 2023-01-01 00:15:35 2023-01-01 00:20:14 2023-01-01 00:20:27   \n",
      "3 2023-01-01 00:35:24 2023-01-01 00:39:30 2023-01-01 00:41:05   \n",
      "4 2023-01-01 00:43:15 2023-01-01 00:51:10 2023-01-01 00:52:47   \n",
      "\n",
      "     dropoff_datetime  PULocationID  DOLocationID  trip_miles  ...  sales_tax  \\\n",
      "0 2023-01-01 00:48:07            48            68        0.94  ...       2.30   \n",
      "1 2023-01-01 01:33:08           246           163        2.78  ...       5.34   \n",
      "2 2023-01-01 00:37:54             9           129        8.81  ...       2.16   \n",
      "3 2023-01-01 00:48:16           129           129        0.67  ...       1.22   \n",
      "4 2023-01-01 01:04:51           129            92        4.38  ...       1.82   \n",
      "\n",
      "   congestion_surcharge  airport_fee  tips  driver_pay  shared_request_flag  \\\n",
      "0                  2.75          0.0  5.22       27.83                    N   \n",
      "1                  2.75          0.0  0.00       50.15                    N   \n",
      "2                  0.00          0.0  0.00       20.22                    N   \n",
      "3                  0.00          0.0  0.00        7.90                    N   \n",
      "4                  0.00          0.0  0.00       16.48                    N   \n",
      "\n",
      "   shared_match_flag  access_a_ride_flag  wav_request_flag wav_match_flag  \n",
      "0                  N                                     N              N  \n",
      "1                  N                                     N              N  \n",
      "2                  N                                     N              N  \n",
      "3                  N                                     N              N  \n",
      "4                  N                                     N              N  \n",
      "\n",
      "[5 rows x 24 columns]\n"
     ]
    }
   ],
   "source": [
    "# load a random High-Volume For-Hire Vehicle trip parquet file to check if the function works correctly\n",
    "example2 = os.path.join(PARQUET_FILES, \"fhvhv_tripdata_2023-01.parquet\")\n",
    "example_df2 = load_parquet_file(example2)\n",
    "\n",
    "# preview the data\n",
    "print(example_df2.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "4c8ea86a-5f05-4c0a-80d8-4c3a22b318bc",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['hvfhs_license_num', 'dispatching_base_num', 'originating_base_num',\n",
       "       'request_datetime', 'on_scene_datetime', 'pickup_datetime',\n",
       "       'dropoff_datetime', 'PULocationID', 'DOLocationID', 'trip_miles',\n",
       "       'trip_time', 'base_passenger_fare', 'tolls', 'bcf', 'sales_tax',\n",
       "       'congestion_surcharge', 'airport_fee', 'tips', 'driver_pay',\n",
       "       'shared_request_flag', 'shared_match_flag', 'access_a_ride_flag',\n",
       "       'wav_request_flag', 'wav_match_flag'],\n",
       "      dtype='object')"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "example_df2.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "054df826-d223-42e1-a063-e8950b87e59e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['VendorID', 'tpep_pickup_datetime', 'tpep_dropoff_datetime',\n",
       "       'passenger_count', 'trip_distance', 'RatecodeID', 'store_and_fwd_flag',\n",
       "       'PULocationID', 'DOLocationID', 'payment_type', 'fare_amount', 'extra',\n",
       "       'mta_tax', 'tip_amount', 'tolls_amount', 'improvement_surcharge',\n",
       "       'total_amount', 'congestion_surcharge', 'airport_fee'],\n",
       "      dtype='object')"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "example_df.columns"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "20e226fb-c4e8-4548-8e42-d30100b34707",
   "metadata": {},
   "source": [
    "### Cleaning and Filtering\n",
    "* Remove all non-Uber data from fhvhv\n",
    "* Remove all invalid pickup and dropoff location IDs for both uber and yellow taxi, where ID is greater than 263 using the `shp` file\n",
    "* Remove unnecessary columns and only keeping columns needed to answer questions in the other parts of this project\n",
    "* Remove invalid data points (use your discretion!)\n",
    "* normalize column names; \n",
    "normalieg and using appropriate column types for the respective dat\n",
    "\n",
    "* Remove trips from both uber and yellow taxi that start and/or end outside of the following latitude/longitude coordinate box: (40.560445, -74.242330) and (40.908524, -73.71704).\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "id": "5acd1bf2-2e19-49a5-a944-9216dcaf0bd8",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\huang\\AppData\\Local\\Temp\\ipykernel_26068\\1589721407.py:2: UserWarning: Geometry is in a geographic CRS. Results from 'centroid' are likely incorrect. Use 'GeoSeries.to_crs()' to re-project geometries to a projected CRS before this operation.\n",
      "\n",
      "  gdf_taxi_zones['centroid'] = gdf_taxi_zones.geometry.centroid\n"
     ]
    }
   ],
   "source": [
    "#Compute the center of the taxi zones for easier comparison and adds a column to the df of our shapefile \n",
    "gdf_taxi_zones['centroid'] = gdf_taxi_zones.geometry.centroid\n",
    "\n",
    "#Removes the bulky geometry column after using it to compute centroid. \n",
    "gdf_taxi_zones = gdf_taxi_zones[['zone','LocationID','centroid']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "id": "f9f09b68-041e-4cac-a0b5-591d043274e1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>zone</th>\n",
       "      <th>LocationID</th>\n",
       "      <th>centroid</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Newark Airport</td>\n",
       "      <td>1</td>\n",
       "      <td>POINT (-74.174 40.69183)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Jamaica Bay</td>\n",
       "      <td>2</td>\n",
       "      <td>POINT (-73.8313 40.61675)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Allerton/Pelham Gardens</td>\n",
       "      <td>3</td>\n",
       "      <td>POINT (-73.84742 40.86447)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Alphabet City</td>\n",
       "      <td>4</td>\n",
       "      <td>POINT (-73.97697 40.72375)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Arden Heights</td>\n",
       "      <td>5</td>\n",
       "      <td>POINT (-74.18848 40.55266)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>258</th>\n",
       "      <td>Woodlawn/Wakefield</td>\n",
       "      <td>259</td>\n",
       "      <td>POINT (-73.85222 40.89793)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>259</th>\n",
       "      <td>Woodside</td>\n",
       "      <td>260</td>\n",
       "      <td>POINT (-73.90631 40.74423)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>260</th>\n",
       "      <td>World Trade Center</td>\n",
       "      <td>261</td>\n",
       "      <td>POINT (-74.01302 40.70914)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>261</th>\n",
       "      <td>Yorkville East</td>\n",
       "      <td>262</td>\n",
       "      <td>POINT (-73.94651 40.77593)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>262</th>\n",
       "      <td>Yorkville West</td>\n",
       "      <td>263</td>\n",
       "      <td>POINT (-73.95101 40.77877)</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>263 rows × 3 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                        zone  LocationID                    centroid\n",
       "0             Newark Airport           1    POINT (-74.174 40.69183)\n",
       "1                Jamaica Bay           2   POINT (-73.8313 40.61675)\n",
       "2    Allerton/Pelham Gardens           3  POINT (-73.84742 40.86447)\n",
       "3              Alphabet City           4  POINT (-73.97697 40.72375)\n",
       "4              Arden Heights           5  POINT (-74.18848 40.55266)\n",
       "..                       ...         ...                         ...\n",
       "258       Woodlawn/Wakefield         259  POINT (-73.85222 40.89793)\n",
       "259                 Woodside         260  POINT (-73.90631 40.74423)\n",
       "260       World Trade Center         261  POINT (-74.01302 40.70914)\n",
       "261           Yorkville East         262  POINT (-73.94651 40.77593)\n",
       "262           Yorkville West         263  POINT (-73.95101 40.77877)\n",
       "\n",
       "[263 rows x 3 columns]"
      ]
     },
     "execution_count": 65,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "gdf_taxi_zones"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "32074561",
   "metadata": {},
   "source": [
    "### Calculate Sample Size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "id": "ae9ffcb8-7268-45ba-af74-752e9710a616",
   "metadata": {},
   "outputs": [],
   "source": [
    "# default: 95% confidence interval, 5% margin of error, p of 0.5 (estimated) proportion of the population which has the attribute in question\n",
    "def cochran_sample_size(population_size):\n",
    "    z_score=1.96\n",
    "    margin_of_error=0.05\n",
    "    p=0.5\n",
    "    sample_size = ((z_score**2)*p*(1-p)) / (margin_of_error**2)\n",
    "    adjusted_sample_size = sample_size / (1 + ((sample_size-1)/population_size))\n",
    "\n",
    "    return int(adjusted_sample_size)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cc33eed4-b2e9-4ab3-94a8-59f04e464c98",
   "metadata": {},
   "source": [
    "### Common Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "id": "f506ca76-802b-414c-a2b0-3389bc66dfb4",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Filter function to remove unecessary rows\n",
    "def filter_data(data):\n",
    "    #Ensure PU and DO locations are within valid location IDs (<= 263)\n",
    "    data = data[(data['PULocationID'] <= 263) & (data['DOLocationID'] <= 263)]\n",
    "    #Filters out rides where PU and DO locations are the same\n",
    "    if 'trip_distance' in data.columns:\n",
    "        filtered_data = data[data['trip_distance'] != 0]\n",
    "\n",
    "    # If 'trip_miles' is present instead of 'trip_distance'\n",
    "    elif 'trip_miles' in data.columns:\n",
    "        filtered_data = data[data['trip_miles'] != 0]\n",
    "        \n",
    "    return filtered_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "id": "8504d167-f1f2-45ee-878f-14a802f71de2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Removes trips from both uber and yellow taxi that start and/or end outside of the following latitude/longitude coordinate box:\n",
    "def find_centroid(data):\n",
    "    LAT_MIN, LON_MIN = 40.560445, -74.242330\n",
    "    LAT_MAX, LON_MAX = 40.908524, -73.717047\n",
    "    \n",
    "    # Extract latitude and longitude from the 'centroid' column using .apply()\n",
    "    data['centroid_lat'] = data['centroid'].apply(lambda point: point.y)\n",
    "    data['centroid_lon'] = data['centroid'].apply(lambda point: point.x)\n",
    "    \n",
    "    # Filter rows where the centroid coordinates are within the bounding coordinate box\n",
    "    centroid_data = data[\n",
    "        (data['centroid_lat'] >= LAT_MIN) & (data['centroid_lat'] <= LAT_MAX) &\n",
    "        (data['centroid_lon'] >= LON_MIN) & (data['centroid_lon'] <= LON_MAX)\n",
    "    ]\n",
    "    return centroid_data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "93daa717",
   "metadata": {},
   "source": [
    "### Process Taxi Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "id": "1a35b7cc",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Grab all of the parquet files in the directory. glob.glob is used to identify/match the pattern, path.join retrieves all the paths \n",
    "all_taxi_parquet_files = glob.glob(os.path.join(PARQUET_FILES, \"*yellow*.parquet\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "id": "1c7f09ed",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\huang\\AppData\\Local\\Temp\\ipykernel_26068\\584579456.py:25: FutureWarning: The behavior of DataFrame concatenation with empty or all-NA entries is deprecated. In a future version, this will no longer exclude empty or all-NA columns when determining the result dtypes. To retain the old behavior, exclude the relevant entries before the concat operation.\n",
      "  sampled_taxi_data = pd.concat(sampled_taxi_dfs)\n"
     ]
    }
   ],
   "source": [
    "#Make a list of just the columns we need for analysis\n",
    "columns_to_keep = [\n",
    "    'tpep_pickup_datetime', 'tpep_dropoff_datetime', 'trip_distance', 'RatecodeID',\n",
    "    'PULocationID', 'DOLocationID', 'fare_amount', 'extra', 'mta_tax',\n",
    "    'tip_amount', 'tolls_amount', 'improvement_surcharge', 'total_amount',\n",
    "    'congestion_surcharge', 'airport_fee'\n",
    "]\n",
    "\n",
    "#Create samples of all taxi parquet files according to cochran's sample size formula. Later, we concatenate all sample dfs into one df. \n",
    "sampled_taxi_dfs = []\n",
    "\n",
    "for file_path in all_taxi_parquet_files:      \n",
    "    taxi_df = load_parquet_file(file_path) #Makes a df for every parquet file \n",
    "    population_size = len(taxi_df)\n",
    "    sample_size = cochran_sample_size(population_size)\n",
    "    sampled_taxi_df = taxi_df.sample(n=sample_size, random_state=42)\n",
    "    #We found that there were a few files that did not have airport_fee as a column. We populate airport_fee with NaN for such parquet files.\n",
    "    for col in columns_to_keep:  \n",
    "        if col not in sampled_taxi_df.columns:\n",
    "            sampled_taxi_df[col] = np.nan \n",
    "    sampled_taxi_df = sampled_taxi_df[columns_to_keep]\n",
    "    sampled_taxi_dfs.append(sampled_taxi_df)\n",
    "\n",
    "    # create one gigantic dataframe with data from every month needed\n",
    "sampled_taxi_data = pd.concat(sampled_taxi_dfs)\n",
    "\n",
    "sampled_taxi_data = filter_data(sampled_taxi_data)\n",
    "\n",
    "# Make a single df that includes the taxi rides and their corresponding coordinates by merging the shape file with the ride files.\n",
    "final_taxi_data = pd.merge(sampled_taxi_data, gdf_taxi_zones, left_on = 'PULocationID', right_on = 'LocationID', how=\"inner\")\n",
    "\n",
    "final_taxi_data = find_centroid(final_taxi_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "id": "10ebd75e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>tpep_pickup_datetime</th>\n",
       "      <th>tpep_dropoff_datetime</th>\n",
       "      <th>trip_distance</th>\n",
       "      <th>RatecodeID</th>\n",
       "      <th>PULocationID</th>\n",
       "      <th>DOLocationID</th>\n",
       "      <th>fare_amount</th>\n",
       "      <th>extra</th>\n",
       "      <th>mta_tax</th>\n",
       "      <th>tip_amount</th>\n",
       "      <th>tolls_amount</th>\n",
       "      <th>improvement_surcharge</th>\n",
       "      <th>total_amount</th>\n",
       "      <th>congestion_surcharge</th>\n",
       "      <th>airport_fee</th>\n",
       "      <th>zone</th>\n",
       "      <th>LocationID</th>\n",
       "      <th>centroid</th>\n",
       "      <th>centroid_lat</th>\n",
       "      <th>centroid_lon</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2020-01-25 10:49:58</td>\n",
       "      <td>2020-01-25 11:07:35</td>\n",
       "      <td>3.28</td>\n",
       "      <td>1.0</td>\n",
       "      <td>142</td>\n",
       "      <td>246</td>\n",
       "      <td>14.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.5</td>\n",
       "      <td>1.70</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.3</td>\n",
       "      <td>19.00</td>\n",
       "      <td>2.5</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Lincoln Square East</td>\n",
       "      <td>142</td>\n",
       "      <td>POINT (-73.98153 40.77363)</td>\n",
       "      <td>40.773633</td>\n",
       "      <td>-73.981532</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2020-01-15 07:30:08</td>\n",
       "      <td>2020-01-15 07:40:01</td>\n",
       "      <td>1.75</td>\n",
       "      <td>1.0</td>\n",
       "      <td>238</td>\n",
       "      <td>166</td>\n",
       "      <td>8.5</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.5</td>\n",
       "      <td>1.20</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.3</td>\n",
       "      <td>13.00</td>\n",
       "      <td>2.5</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Upper West Side North</td>\n",
       "      <td>238</td>\n",
       "      <td>POINT (-73.97305 40.7917)</td>\n",
       "      <td>40.791705</td>\n",
       "      <td>-73.973049</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2020-01-09 06:29:09</td>\n",
       "      <td>2020-01-09 06:35:44</td>\n",
       "      <td>0.87</td>\n",
       "      <td>1.0</td>\n",
       "      <td>100</td>\n",
       "      <td>164</td>\n",
       "      <td>5.5</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.5</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.3</td>\n",
       "      <td>8.80</td>\n",
       "      <td>2.5</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Garment District</td>\n",
       "      <td>100</td>\n",
       "      <td>POINT (-73.98879 40.75351)</td>\n",
       "      <td>40.753513</td>\n",
       "      <td>-73.988787</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2020-01-26 12:24:04</td>\n",
       "      <td>2020-01-26 12:29:15</td>\n",
       "      <td>0.98</td>\n",
       "      <td>1.0</td>\n",
       "      <td>161</td>\n",
       "      <td>43</td>\n",
       "      <td>5.5</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.5</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.3</td>\n",
       "      <td>8.80</td>\n",
       "      <td>2.5</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Midtown Center</td>\n",
       "      <td>161</td>\n",
       "      <td>POINT (-73.9777 40.75803)</td>\n",
       "      <td>40.758028</td>\n",
       "      <td>-73.977698</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>2020-01-30 07:57:53</td>\n",
       "      <td>2020-01-30 08:10:19</td>\n",
       "      <td>1.30</td>\n",
       "      <td>1.0</td>\n",
       "      <td>229</td>\n",
       "      <td>262</td>\n",
       "      <td>9.0</td>\n",
       "      <td>2.5</td>\n",
       "      <td>0.5</td>\n",
       "      <td>2.45</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.3</td>\n",
       "      <td>14.75</td>\n",
       "      <td>2.5</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Sutton Place/Turtle Bay North</td>\n",
       "      <td>229</td>\n",
       "      <td>POINT (-73.96515 40.75673)</td>\n",
       "      <td>40.756729</td>\n",
       "      <td>-73.965146</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  tpep_pickup_datetime tpep_dropoff_datetime  trip_distance  RatecodeID  \\\n",
       "0  2020-01-25 10:49:58   2020-01-25 11:07:35           3.28         1.0   \n",
       "1  2020-01-15 07:30:08   2020-01-15 07:40:01           1.75         1.0   \n",
       "2  2020-01-09 06:29:09   2020-01-09 06:35:44           0.87         1.0   \n",
       "3  2020-01-26 12:24:04   2020-01-26 12:29:15           0.98         1.0   \n",
       "4  2020-01-30 07:57:53   2020-01-30 08:10:19           1.30         1.0   \n",
       "\n",
       "   PULocationID  DOLocationID  fare_amount  extra  mta_tax  tip_amount  \\\n",
       "0           142           246         14.0    0.0      0.5        1.70   \n",
       "1           238           166          8.5    0.0      0.5        1.20   \n",
       "2           100           164          5.5    0.0      0.5        0.00   \n",
       "3           161            43          5.5    0.0      0.5        0.00   \n",
       "4           229           262          9.0    2.5      0.5        2.45   \n",
       "\n",
       "   tolls_amount  improvement_surcharge  total_amount  congestion_surcharge  \\\n",
       "0           0.0                    0.3         19.00                   2.5   \n",
       "1           0.0                    0.3         13.00                   2.5   \n",
       "2           0.0                    0.3          8.80                   2.5   \n",
       "3           0.0                    0.3          8.80                   2.5   \n",
       "4           0.0                    0.3         14.75                   2.5   \n",
       "\n",
       "   airport_fee                           zone  LocationID  \\\n",
       "0          NaN            Lincoln Square East         142   \n",
       "1          NaN          Upper West Side North         238   \n",
       "2          NaN               Garment District         100   \n",
       "3          NaN                 Midtown Center         161   \n",
       "4          NaN  Sutton Place/Turtle Bay North         229   \n",
       "\n",
       "                     centroid  centroid_lat  centroid_lon  \n",
       "0  POINT (-73.98153 40.77363)     40.773633    -73.981532  \n",
       "1   POINT (-73.97305 40.7917)     40.791705    -73.973049  \n",
       "2  POINT (-73.98879 40.75351)     40.753513    -73.988787  \n",
       "3   POINT (-73.9777 40.75803)     40.758028    -73.977698  \n",
       "4  POINT (-73.96515 40.75673)     40.756729    -73.965146  "
      ]
     },
     "execution_count": 101,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "final_taxi_data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "id": "c9da7089-3f6b-4f93-a22e-76bf554daca8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "Index: 16744 entries, 0 to 17067\n",
      "Data columns (total 20 columns):\n",
      " #   Column                 Non-Null Count  Dtype         \n",
      "---  ------                 --------------  -----         \n",
      " 0   tpep_pickup_datetime   16744 non-null  datetime64[us]\n",
      " 1   tpep_dropoff_datetime  16744 non-null  datetime64[us]\n",
      " 2   trip_distance          16744 non-null  float64       \n",
      " 3   RatecodeID             16012 non-null  float64       \n",
      " 4   PULocationID           16744 non-null  int64         \n",
      " 5   DOLocationID           16744 non-null  int64         \n",
      " 6   fare_amount            16744 non-null  float64       \n",
      " 7   extra                  16744 non-null  float64       \n",
      " 8   mta_tax                16744 non-null  float64       \n",
      " 9   tip_amount             16744 non-null  float64       \n",
      " 10  tolls_amount           16744 non-null  float64       \n",
      " 11  improvement_surcharge  16744 non-null  float64       \n",
      " 12  total_amount           16744 non-null  float64       \n",
      " 13  congestion_surcharge   16012 non-null  float64       \n",
      " 14  airport_fee            7947 non-null   float64       \n",
      " 15  zone                   16744 non-null  object        \n",
      " 16  LocationID             16744 non-null  int32         \n",
      " 17  centroid               16744 non-null  geometry      \n",
      " 18  centroid_lat           16744 non-null  float64       \n",
      " 19  centroid_lon           16744 non-null  float64       \n",
      "dtypes: datetime64[us](2), float64(13), geometry(1), int32(1), int64(2), object(1)\n",
      "memory usage: 2.6+ MB\n"
     ]
    }
   ],
   "source": [
    "final_taxi_data.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "id": "50c85e25-6416-4c16-b98c-09596cdc6865",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>tpep_pickup_datetime</th>\n",
       "      <th>tpep_dropoff_datetime</th>\n",
       "      <th>trip_distance</th>\n",
       "      <th>RatecodeID</th>\n",
       "      <th>PULocationID</th>\n",
       "      <th>DOLocationID</th>\n",
       "      <th>fare_amount</th>\n",
       "      <th>extra</th>\n",
       "      <th>mta_tax</th>\n",
       "      <th>tip_amount</th>\n",
       "      <th>tolls_amount</th>\n",
       "      <th>improvement_surcharge</th>\n",
       "      <th>total_amount</th>\n",
       "      <th>congestion_surcharge</th>\n",
       "      <th>airport_fee</th>\n",
       "      <th>LocationID</th>\n",
       "      <th>centroid_lat</th>\n",
       "      <th>centroid_lon</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>count</th>\n",
       "      <td>16744</td>\n",
       "      <td>16744</td>\n",
       "      <td>16744.000000</td>\n",
       "      <td>16012.000000</td>\n",
       "      <td>16744.000000</td>\n",
       "      <td>16744.000000</td>\n",
       "      <td>16744.000000</td>\n",
       "      <td>16744.000000</td>\n",
       "      <td>16744.000000</td>\n",
       "      <td>16744.000000</td>\n",
       "      <td>16744.000000</td>\n",
       "      <td>16744.000000</td>\n",
       "      <td>16744.000000</td>\n",
       "      <td>16012.000000</td>\n",
       "      <td>7947.000000</td>\n",
       "      <td>16744.000000</td>\n",
       "      <td>16744.000000</td>\n",
       "      <td>16744.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mean</th>\n",
       "      <td>2021-11-15 08:41:08.601588</td>\n",
       "      <td>2021-11-15 08:57:04.603678</td>\n",
       "      <td>3.291102</td>\n",
       "      <td>1.199663</td>\n",
       "      <td>163.774546</td>\n",
       "      <td>160.121715</td>\n",
       "      <td>14.395644</td>\n",
       "      <td>1.156294</td>\n",
       "      <td>0.491728</td>\n",
       "      <td>2.531626</td>\n",
       "      <td>0.415078</td>\n",
       "      <td>0.438915</td>\n",
       "      <td>21.058254</td>\n",
       "      <td>2.297652</td>\n",
       "      <td>0.086825</td>\n",
       "      <td>163.774546</td>\n",
       "      <td>40.753793</td>\n",
       "      <td>-73.966838</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>min</th>\n",
       "      <td>2020-01-01 00:11:06</td>\n",
       "      <td>2020-01-01 00:30:50</td>\n",
       "      <td>0.010000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>4.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>-81.520000</td>\n",
       "      <td>-7.500000</td>\n",
       "      <td>-0.500000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>-6.550000</td>\n",
       "      <td>-1.000000</td>\n",
       "      <td>-80.050000</td>\n",
       "      <td>-2.500000</td>\n",
       "      <td>-1.250000</td>\n",
       "      <td>4.000000</td>\n",
       "      <td>40.576961</td>\n",
       "      <td>-74.029892</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25%</th>\n",
       "      <td>2020-12-08 16:10:13</td>\n",
       "      <td>2020-12-08 16:15:40</td>\n",
       "      <td>1.100000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>132.000000</td>\n",
       "      <td>107.000000</td>\n",
       "      <td>7.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.500000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.300000</td>\n",
       "      <td>12.250000</td>\n",
       "      <td>2.500000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>132.000000</td>\n",
       "      <td>40.740439</td>\n",
       "      <td>-73.989845</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50%</th>\n",
       "      <td>2021-11-15 15:59:40.500000</td>\n",
       "      <td>2021-11-15 16:12:57.500000</td>\n",
       "      <td>1.830000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>162.000000</td>\n",
       "      <td>161.000000</td>\n",
       "      <td>10.000000</td>\n",
       "      <td>0.500000</td>\n",
       "      <td>0.500000</td>\n",
       "      <td>2.060000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.300000</td>\n",
       "      <td>15.960000</td>\n",
       "      <td>2.500000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>162.000000</td>\n",
       "      <td>40.758028</td>\n",
       "      <td>-73.977698</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>75%</th>\n",
       "      <td>2022-10-22 21:01:30.500000</td>\n",
       "      <td>2022-10-22 21:11:01.750000</td>\n",
       "      <td>3.360000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>234.000000</td>\n",
       "      <td>234.000000</td>\n",
       "      <td>16.300000</td>\n",
       "      <td>2.500000</td>\n",
       "      <td>0.500000</td>\n",
       "      <td>3.240000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.300000</td>\n",
       "      <td>23.100000</td>\n",
       "      <td>2.500000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>234.000000</td>\n",
       "      <td>40.773633</td>\n",
       "      <td>-73.959635</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>max</th>\n",
       "      <td>2023-09-30 23:06:46</td>\n",
       "      <td>2023-09-30 23:51:36</td>\n",
       "      <td>67.900000</td>\n",
       "      <td>99.000000</td>\n",
       "      <td>263.000000</td>\n",
       "      <td>263.000000</td>\n",
       "      <td>209.500000</td>\n",
       "      <td>11.750000</td>\n",
       "      <td>0.500000</td>\n",
       "      <td>50.000000</td>\n",
       "      <td>40.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>262.700000</td>\n",
       "      <td>2.500000</td>\n",
       "      <td>1.250000</td>\n",
       "      <td>263.000000</td>\n",
       "      <td>40.899529</td>\n",
       "      <td>-73.739337</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>std</th>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>4.122359</td>\n",
       "      <td>3.952569</td>\n",
       "      <td>65.458809</td>\n",
       "      <td>70.814713</td>\n",
       "      <td>13.229554</td>\n",
       "      <td>1.428814</td>\n",
       "      <td>0.083810</td>\n",
       "      <td>2.949458</td>\n",
       "      <td>1.751952</td>\n",
       "      <td>0.293415</td>\n",
       "      <td>16.677861</td>\n",
       "      <td>0.728378</td>\n",
       "      <td>0.321505</td>\n",
       "      <td>65.458809</td>\n",
       "      <td>0.032691</td>\n",
       "      <td>0.044686</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "             tpep_pickup_datetime       tpep_dropoff_datetime  trip_distance  \\\n",
       "count                       16744                       16744   16744.000000   \n",
       "mean   2021-11-15 08:41:08.601588  2021-11-15 08:57:04.603678       3.291102   \n",
       "min           2020-01-01 00:11:06         2020-01-01 00:30:50       0.010000   \n",
       "25%           2020-12-08 16:10:13         2020-12-08 16:15:40       1.100000   \n",
       "50%    2021-11-15 15:59:40.500000  2021-11-15 16:12:57.500000       1.830000   \n",
       "75%    2022-10-22 21:01:30.500000  2022-10-22 21:11:01.750000       3.360000   \n",
       "max           2023-09-30 23:06:46         2023-09-30 23:51:36      67.900000   \n",
       "std                           NaN                         NaN       4.122359   \n",
       "\n",
       "         RatecodeID  PULocationID  DOLocationID   fare_amount         extra  \\\n",
       "count  16012.000000  16744.000000  16744.000000  16744.000000  16744.000000   \n",
       "mean       1.199663    163.774546    160.121715     14.395644      1.156294   \n",
       "min        1.000000      4.000000      1.000000    -81.520000     -7.500000   \n",
       "25%        1.000000    132.000000    107.000000      7.000000      0.000000   \n",
       "50%        1.000000    162.000000    161.000000     10.000000      0.500000   \n",
       "75%        1.000000    234.000000    234.000000     16.300000      2.500000   \n",
       "max       99.000000    263.000000    263.000000    209.500000     11.750000   \n",
       "std        3.952569     65.458809     70.814713     13.229554      1.428814   \n",
       "\n",
       "            mta_tax    tip_amount  tolls_amount  improvement_surcharge  \\\n",
       "count  16744.000000  16744.000000  16744.000000           16744.000000   \n",
       "mean       0.491728      2.531626      0.415078               0.438915   \n",
       "min       -0.500000      0.000000     -6.550000              -1.000000   \n",
       "25%        0.500000      0.000000      0.000000               0.300000   \n",
       "50%        0.500000      2.060000      0.000000               0.300000   \n",
       "75%        0.500000      3.240000      0.000000               0.300000   \n",
       "max        0.500000     50.000000     40.000000               1.000000   \n",
       "std        0.083810      2.949458      1.751952               0.293415   \n",
       "\n",
       "       total_amount  congestion_surcharge  airport_fee    LocationID  \\\n",
       "count  16744.000000          16012.000000  7947.000000  16744.000000   \n",
       "mean      21.058254              2.297652     0.086825    163.774546   \n",
       "min      -80.050000             -2.500000    -1.250000      4.000000   \n",
       "25%       12.250000              2.500000     0.000000    132.000000   \n",
       "50%       15.960000              2.500000     0.000000    162.000000   \n",
       "75%       23.100000              2.500000     0.000000    234.000000   \n",
       "max      262.700000              2.500000     1.250000    263.000000   \n",
       "std       16.677861              0.728378     0.321505     65.458809   \n",
       "\n",
       "       centroid_lat  centroid_lon  \n",
       "count  16744.000000  16744.000000  \n",
       "mean      40.753793    -73.966838  \n",
       "min       40.576961    -74.029892  \n",
       "25%       40.740439    -73.989845  \n",
       "50%       40.758028    -73.977698  \n",
       "75%       40.773633    -73.959635  \n",
       "max       40.899529    -73.739337  \n",
       "std        0.032691      0.044686  "
      ]
     },
     "execution_count": 78,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "final_taxi_data.describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "094b4d6d",
   "metadata": {},
   "source": [
    "### Processing Uber Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "id": "0d0658d4-173f-4290-abd8-022b5b9e5560",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Grab all of the parquet files in the directory. glob.glob is used to identify/match the pattern, path.join retrieves all the paths \n",
    "all_fhvhv_parquet_files = glob.glob(os.path.join(PARQUET_FILES, \"*fhvhv*.parquet\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "id": "4c4a1fb4-01b4-43ff-b5e7-492ce2fc3bc2",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\huang\\AppData\\Local\\Temp\\ipykernel_26068\\2112851013.py:19: FutureWarning: The behavior of DataFrame concatenation with empty or all-NA entries is deprecated. In a future version, this will no longer exclude empty or all-NA columns when determining the result dtypes. To retain the old behavior, exclude the relevant entries before the concat operation.\n",
      "  uber_data = pd.concat(sampled_uber_dfs)\n"
     ]
    }
   ],
   "source": [
    "#Create samples of all uber parquet files according to cochran's sample size formula. Later, we concatenate all sample dfs into one df. \n",
    "sampled_uber_dfs = []\n",
    "columns_to_keep = ['hvfhs_license_num',\n",
    "       'request_datetime', 'pickup_datetime',\n",
    "       'dropoff_datetime', 'PULocationID', 'DOLocationID', 'trip_miles',\n",
    "        'base_passenger_fare', 'tolls', 'bcf', 'sales_tax',\n",
    "       'congestion_surcharge', 'airport_fee', 'tips', 'driver_pay']\n",
    "\n",
    "for file_path in all_fhvhv_parquet_files:      \n",
    "    uber_df = load_parquet_file(file_path) #Makes a df for every parquet file \n",
    "    uber_df = uber_df[uber_df['hvfhs_license_num'] == 'HV0003'] #Filters out non-uber rides from the hvfhs files before creating samples\n",
    "    population_size = len(uber_df)\n",
    "    sample_size = cochran_sample_size(population_size)\n",
    "    sampled_uber_df = uber_df.sample(n=sample_size, random_state=42)\n",
    "    sampled_uber_df = sampled_uber_df[columns_to_keep]\n",
    "    sampled_uber_dfs.append(sampled_uber_df)\n",
    "\n",
    "    # create one gigantic dataframe with data from every month needed\n",
    "sampled_uber_data = pd.concat(sampled_uber_dfs)\n",
    "\n",
    "sampled_uber_data = filter_data(sampled_uber_data)\n",
    "\n",
    "# Make a single df that includes the taxi rides and their corresponding coordinates by merging the shape file with the ride files.\n",
    "final_uber_data = pd.merge(sampled_uber_data, gdf_taxi_zones, left_on = 'PULocationID', right_on = 'LocationID', how=\"inner\")\n",
    "\n",
    "final_uber_data = find_centroid(final_uber_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "339997e2",
   "metadata": {},
   "outputs": [],
   "source": [
    "final_uber_data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "74d783db-e527-4847-bf70-2d7428ea3897",
   "metadata": {},
   "outputs": [],
   "source": [
    "final_uber_data.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6fddeb14-cd70-4e83-8f93-974642c3bea3",
   "metadata": {},
   "outputs": [],
   "source": [
    "final_uber_data.describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "45a15cbb",
   "metadata": {},
   "source": [
    "### Processing Weather Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "id": "0ec5370f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_all_weather_csvs(directory):\n",
    "    weather_dfs = []\n",
    "\n",
    "    # Iterate over all files in the given directory\n",
    "    for filename in os.listdir(directory):\n",
    "        file_path = os.path.join(directory, filename)\n",
    "        # Read the CSV file into a DataFrame\n",
    "        df = pd.read_csv(file_path, low_memory=False)\n",
    "        # Append the DataFrame to the list\n",
    "        weather_dfs.append(df)\n",
    "    weather_dfs = pd.concat(weather_dfs, ignore_index=True)\n",
    "    return weather_dfs\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "id": "4ff3a175-10a4-4187-88d7-240325c3a672",
   "metadata": {},
   "outputs": [
    {
     "ename": "UnicodeDecodeError",
     "evalue": "'utf-8' codec can't decode byte 0xff in position 580: invalid start byte",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mUnicodeDecodeError\u001b[0m                        Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[94], line 3\u001b[0m\n\u001b[1;32m      1\u001b[0m columns_to_keep \u001b[38;5;241m=\u001b[39m [\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mDATE\u001b[39m\u001b[38;5;124m'\u001b[39m,\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mLATITUDE\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mLONGITUDE\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mMonthlyTotalLiquidPrecipitation\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mDailyPrecipitation\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mHourlyPrecipitation\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mDailyAverageWindSpeed\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mHourlyWindSpeed\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mDailySnowfall\u001b[39m\u001b[38;5;124m'\u001b[39m]\n\u001b[0;32m----> 3\u001b[0m weather_data \u001b[38;5;241m=\u001b[39m get_all_weather_csvs(WEATHER_CSV_DIR)\n\u001b[1;32m      4\u001b[0m weather_data \u001b[38;5;241m=\u001b[39m weather_data[columns_to_keep]\n\u001b[1;32m      5\u001b[0m weather_data[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mDATE\u001b[39m\u001b[38;5;124m'\u001b[39m] \u001b[38;5;241m=\u001b[39m pd\u001b[38;5;241m.\u001b[39mto_datetime(weather_data[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mDATE\u001b[39m\u001b[38;5;124m'\u001b[39m])\n",
      "Cell \u001b[0;32mIn[93], line 8\u001b[0m, in \u001b[0;36mget_all_weather_csvs\u001b[0;34m(directory)\u001b[0m\n\u001b[1;32m      6\u001b[0m file_path \u001b[38;5;241m=\u001b[39m os\u001b[38;5;241m.\u001b[39mpath\u001b[38;5;241m.\u001b[39mjoin(directory, filename)\n\u001b[1;32m      7\u001b[0m \u001b[38;5;66;03m# Read the CSV file into a DataFrame\u001b[39;00m\n\u001b[0;32m----> 8\u001b[0m df \u001b[38;5;241m=\u001b[39m pd\u001b[38;5;241m.\u001b[39mread_csv(file_path, low_memory\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m)\n\u001b[1;32m      9\u001b[0m \u001b[38;5;66;03m# Append the DataFrame to the list\u001b[39;00m\n\u001b[1;32m     10\u001b[0m weather_dfs\u001b[38;5;241m.\u001b[39mappend(df)\n",
      "File \u001b[0;32m/opt/anaconda3/lib/python3.12/site-packages/pandas/io/parsers/readers.py:1026\u001b[0m, in \u001b[0;36mread_csv\u001b[0;34m(filepath_or_buffer, sep, delimiter, header, names, index_col, usecols, dtype, engine, converters, true_values, false_values, skipinitialspace, skiprows, skipfooter, nrows, na_values, keep_default_na, na_filter, verbose, skip_blank_lines, parse_dates, infer_datetime_format, keep_date_col, date_parser, date_format, dayfirst, cache_dates, iterator, chunksize, compression, thousands, decimal, lineterminator, quotechar, quoting, doublequote, escapechar, comment, encoding, encoding_errors, dialect, on_bad_lines, delim_whitespace, low_memory, memory_map, float_precision, storage_options, dtype_backend)\u001b[0m\n\u001b[1;32m   1013\u001b[0m kwds_defaults \u001b[38;5;241m=\u001b[39m _refine_defaults_read(\n\u001b[1;32m   1014\u001b[0m     dialect,\n\u001b[1;32m   1015\u001b[0m     delimiter,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   1022\u001b[0m     dtype_backend\u001b[38;5;241m=\u001b[39mdtype_backend,\n\u001b[1;32m   1023\u001b[0m )\n\u001b[1;32m   1024\u001b[0m kwds\u001b[38;5;241m.\u001b[39mupdate(kwds_defaults)\n\u001b[0;32m-> 1026\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m _read(filepath_or_buffer, kwds)\n",
      "File \u001b[0;32m/opt/anaconda3/lib/python3.12/site-packages/pandas/io/parsers/readers.py:620\u001b[0m, in \u001b[0;36m_read\u001b[0;34m(filepath_or_buffer, kwds)\u001b[0m\n\u001b[1;32m    617\u001b[0m _validate_names(kwds\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mnames\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;28;01mNone\u001b[39;00m))\n\u001b[1;32m    619\u001b[0m \u001b[38;5;66;03m# Create the parser.\u001b[39;00m\n\u001b[0;32m--> 620\u001b[0m parser \u001b[38;5;241m=\u001b[39m TextFileReader(filepath_or_buffer, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwds)\n\u001b[1;32m    622\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m chunksize \u001b[38;5;129;01mor\u001b[39;00m iterator:\n\u001b[1;32m    623\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m parser\n",
      "File \u001b[0;32m/opt/anaconda3/lib/python3.12/site-packages/pandas/io/parsers/readers.py:1620\u001b[0m, in \u001b[0;36mTextFileReader.__init__\u001b[0;34m(self, f, engine, **kwds)\u001b[0m\n\u001b[1;32m   1617\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39moptions[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mhas_index_names\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m kwds[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mhas_index_names\u001b[39m\u001b[38;5;124m\"\u001b[39m]\n\u001b[1;32m   1619\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mhandles: IOHandles \u001b[38;5;241m|\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m-> 1620\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_engine \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_make_engine(f, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mengine)\n",
      "File \u001b[0;32m/opt/anaconda3/lib/python3.12/site-packages/pandas/io/parsers/readers.py:1898\u001b[0m, in \u001b[0;36mTextFileReader._make_engine\u001b[0;34m(self, f, engine)\u001b[0m\n\u001b[1;32m   1895\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(msg)\n\u001b[1;32m   1897\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m-> 1898\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m mapping[engine](f, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39moptions)\n\u001b[1;32m   1899\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m:\n\u001b[1;32m   1900\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mhandles \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n",
      "File \u001b[0;32m/opt/anaconda3/lib/python3.12/site-packages/pandas/io/parsers/c_parser_wrapper.py:93\u001b[0m, in \u001b[0;36mCParserWrapper.__init__\u001b[0;34m(self, src, **kwds)\u001b[0m\n\u001b[1;32m     90\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m kwds[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mdtype_backend\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mpyarrow\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n\u001b[1;32m     91\u001b[0m     \u001b[38;5;66;03m# Fail here loudly instead of in cython after reading\u001b[39;00m\n\u001b[1;32m     92\u001b[0m     import_optional_dependency(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mpyarrow\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m---> 93\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_reader \u001b[38;5;241m=\u001b[39m parsers\u001b[38;5;241m.\u001b[39mTextReader(src, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwds)\n\u001b[1;32m     95\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39munnamed_cols \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_reader\u001b[38;5;241m.\u001b[39munnamed_cols\n\u001b[1;32m     97\u001b[0m \u001b[38;5;66;03m# error: Cannot determine type of 'names'\u001b[39;00m\n",
      "File \u001b[0;32mparsers.pyx:574\u001b[0m, in \u001b[0;36mpandas._libs.parsers.TextReader.__cinit__\u001b[0;34m()\u001b[0m\n",
      "File \u001b[0;32mparsers.pyx:663\u001b[0m, in \u001b[0;36mpandas._libs.parsers.TextReader._get_header\u001b[0;34m()\u001b[0m\n",
      "File \u001b[0;32mparsers.pyx:874\u001b[0m, in \u001b[0;36mpandas._libs.parsers.TextReader._tokenize_rows\u001b[0;34m()\u001b[0m\n",
      "File \u001b[0;32mparsers.pyx:891\u001b[0m, in \u001b[0;36mpandas._libs.parsers.TextReader._check_tokenize_status\u001b[0;34m()\u001b[0m\n",
      "File \u001b[0;32mparsers.pyx:2053\u001b[0m, in \u001b[0;36mpandas._libs.parsers.raise_parser_error\u001b[0;34m()\u001b[0m\n",
      "File \u001b[0;32m<frozen codecs>:322\u001b[0m, in \u001b[0;36mdecode\u001b[0;34m(self, input, final)\u001b[0m\n",
      "\u001b[0;31mUnicodeDecodeError\u001b[0m: 'utf-8' codec can't decode byte 0xff in position 580: invalid start byte"
     ]
    }
   ],
   "source": [
    "columns_to_keep = ['DATE','LATITUDE', 'LONGITUDE', 'MonthlyTotalLiquidPrecipitation', 'DailyPrecipitation', 'HourlyPrecipitation', 'DailyAverageWindSpeed', 'HourlyWindSpeed', 'DailySnowfall']\n",
    "\n",
    "weather_data = get_all_weather_csvs(WEATHER_CSV_DIR)\n",
    "weather_data = weather_data[columns_to_keep]\n",
    "weather_data['DATE'] = pd.to_datetime(weather_data['DATE'])\n",
    "\n",
    "# weather_data[weather_data[\"DailyPrecipitation\"].isna()] #54343\n",
    "weather_data[weather_data[\"HourlyPrecipitation\"].isna()] #8,523\n",
    "# weather_data\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "id": "4b32ee20",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Empty DataFrame\n",
      "Columns: [DATE, LATITUDE, LONGITUDE, MonthlyTotalLiquidPrecipitation, DailyPrecipitation, HourlyPrecipitation, DailyAverageWindSpeed, HourlyWindSpeed, DailySnowfall, DATE_ONLY]\n",
      "Index: []\n",
      "Unique String Values and Their Counts in HourlyPrecipitation:\n",
      "Series([], Name: count, dtype: int64)\n"
     ]
    }
   ],
   "source": [
    "#We needed to do this because an error was being thrown that the hourly precipitation values were strings. \n",
    "# Identify rows where HourlyPrecipitation is a string\n",
    "string_precipitation_rows = weather_data[weather_data['HourlyPrecipitation'].apply(lambda x: isinstance(x, str))]\n",
    "\n",
    "# Print or display the rows to inspect the problematic values\n",
    "print(string_precipitation_rows)\n",
    "# Identify rows where HourlyPrecipitation is a string\n",
    "string_precipitation_rows = weather_data[weather_data['HourlyPrecipitation'].apply(lambda x: isinstance(x, str))]\n",
    "\n",
    "# Extract the HourlyPrecipitation values that are strings\n",
    "string_values = string_precipitation_rows['HourlyPrecipitation']\n",
    "\n",
    "# Get unique values and their count using value_counts()\n",
    "string_value_counts = string_values.value_counts()\n",
    "\n",
    "# Print or display the unique string values and their count\n",
    "print(\"Unique String Values and Their Counts in HourlyPrecipitation:\")\n",
    "print(string_value_counts)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "id": "8e18763d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                 DATE  LATITUDE  LONGITUDE MonthlyTotalLiquidPrecipitation  \\\n",
      "0 2020-01-01 00:51:00  40.77898  -73.96925                             NaN   \n",
      "1 2020-01-01 01:51:00  40.77898  -73.96925                             NaN   \n",
      "2 2020-01-01 02:51:00  40.77898  -73.96925                             NaN   \n",
      "3 2020-01-01 03:51:00  40.77898  -73.96925                             NaN   \n",
      "4 2020-01-01 04:51:00  40.77898  -73.96925                             NaN   \n",
      "\n",
      "  DailyPrecipitation  HourlyPrecipitation  DailyAverageWindSpeed  \\\n",
      "0                NaN                  0.0                    NaN   \n",
      "1                NaN                  0.0                    NaN   \n",
      "2                NaN                  0.0                    NaN   \n",
      "3                NaN                  0.0                    NaN   \n",
      "4                NaN                  0.0                    NaN   \n",
      "\n",
      "   HourlyWindSpeed DailySnowfall   DATE_ONLY  \n",
      "0              8.0           NaN  2020-01-01  \n",
      "1              8.0           NaN  2020-01-01  \n",
      "2             14.0           NaN  2020-01-01  \n",
      "3             11.0           NaN  2020-01-01  \n",
      "4              6.0           NaN  2020-01-01  \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/wc/z6g_m0_n659dtcmkkgf76pv40000gn/T/ipykernel_30577/652454805.py:8: FutureWarning: A value is trying to be set on a copy of a DataFrame or Series through chained assignment using an inplace method.\n",
      "The behavior will change in pandas 3.0. This inplace method will never work because the intermediate object on which we are setting values always behaves as a copy.\n",
      "\n",
      "For example, when doing 'df[col].method(value, inplace=True)', try using 'df.method({col: value}, inplace=True)' or df[col] = df[col].method(value) instead, to perform the operation inplace on the original object.\n",
      "\n",
      "\n",
      "  weather_data['HourlyPrecipitation'].replace('T', 0.00, inplace=True)\n"
     ]
    }
   ],
   "source": [
    "#Cleans out the string values based on different cases identified in the query above\n",
    "# Replace 'T' (trace amount) with 0.00\n",
    "weather_data['HourlyPrecipitation'].replace('T', 0.00, inplace=True)\n",
    "\n",
    "# Function to clean non-numeric characters from values\n",
    "def clean_precipitation_value(value):\n",
    "    if isinstance(value, str):\n",
    "        # Remove all non-numeric characters except the decimal point\n",
    "        return re.sub(r'[^0-9.]', '', value)\n",
    "    return value\n",
    "\n",
    "# Apply the cleaning function to the HourlyPrecipitation column\n",
    "weather_data['HourlyPrecipitation'] = weather_data['HourlyPrecipitation'].apply(clean_precipitation_value)\n",
    "\n",
    "# Convert all cleaned values to numeric, coercing any problematic ones to NaN\n",
    "weather_data['HourlyPrecipitation'] = pd.to_numeric(weather_data['HourlyPrecipitation'], errors='coerce')\n",
    "\n",
    "# Display the cleaned DataFrame\n",
    "print(weather_data.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "id": "42534e10",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fill in the remaining null hourly precipiation values \n",
    "# Ensure that 'DATE' is in the correct datetime format\n",
    "weather_data['DATE'] = pd.to_datetime(weather_data['DATE'])\n",
    "\n",
    "# Add a column for just the date (without time), which will help with grouping\n",
    "weather_data['DATE_ONLY'] = weather_data['DATE'].dt.date\n",
    "# Group by each unique date\n",
    "daily_groups = weather_data.groupby('DATE_ONLY')\n",
    "\n",
    "# Loop over each group of a specific date and calculate the remaining precipitation to distribute\n",
    "for date, group in daily_groups:\n",
    "    # Get the daily precipitation for the current day\n",
    "    daily_precipitation = group['DailyPrecipitation'].iloc[0]\n",
    "    \n",
    "    # Calculate the sum of existing hourly precipitation values (if any)\n",
    "    existing_hourly_precip = group['HourlyPrecipitation'].sum(skipna=True)\n",
    "\n",
    "    # Calculate the remaining precipitation that needs to be distributed to missing hours\n",
    "    remaining_precip = daily_precipitation - existing_hourly_precip\n",
    "\n",
    "    # Get the number of hours with missing HourlyPrecipitation\n",
    "    missing_hours = group['HourlyPrecipitation'].isna().sum()\n",
    "\n",
    "    # Calculate how much to distribute to each missing hour\n",
    "    if missing_hours > 0 and remaining_precip > 0:\n",
    "        hourly_precipitation_to_assign = remaining_precip / missing_hours\n",
    "    else:\n",
    "        hourly_precipitation_to_assign = 0\n",
    "\n",
    "    # Fill missing HourlyPrecipitation values with the calculated amount\n",
    "    weather_data.loc[group.index, 'HourlyPrecipitation'] = group['HourlyPrecipitation'].fillna(hourly_precipitation_to_assign)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "id": "79d7f715",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>DATE</th>\n",
       "      <th>LATITUDE</th>\n",
       "      <th>LONGITUDE</th>\n",
       "      <th>MonthlyTotalLiquidPrecipitation</th>\n",
       "      <th>DailyPrecipitation</th>\n",
       "      <th>HourlyPrecipitation</th>\n",
       "      <th>DailyAverageWindSpeed</th>\n",
       "      <th>HourlyWindSpeed</th>\n",
       "      <th>DailySnowfall</th>\n",
       "      <th>DATE_ONLY</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "Empty DataFrame\n",
       "Columns: [DATE, LATITUDE, LONGITUDE, MonthlyTotalLiquidPrecipitation, DailyPrecipitation, HourlyPrecipitation, DailyAverageWindSpeed, HourlyWindSpeed, DailySnowfall, DATE_ONLY]\n",
       "Index: []"
      ]
     },
     "execution_count": 106,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Confirm there are no null hourly precipitation values anymore \n",
    "weather_data[weather_data[\"HourlyPrecipitation\"].isna()] #0 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "76e864ab",
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean_month_weather_data_hourly(csv_file):\n",
    "    raise NotImplementedError()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0687581f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean_month_weather_data_daily(csv_file):\n",
    "    raise NotImplementedError()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3ef8945d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_and_clean_weather_data():\n",
    "    weather_csv_files = get_all_weather_csvs(WEATHER_CSV_DIR)\n",
    "    \n",
    "    hourly_dataframes = []\n",
    "    daily_dataframes = []\n",
    "        \n",
    "    for csv_file in weather_csv_files:\n",
    "        hourly_dataframe = clean_month_weather_data_hourly(csv_file)\n",
    "        daily_dataframe = clean_month_weather_data_daily(csv_file)\n",
    "        hourly_dataframes.append(hourly_dataframe)\n",
    "        daily_dataframes.append(daily_dataframe)\n",
    "        \n",
    "    # create two dataframes with hourly & daily data from every month\n",
    "    hourly_data = pd.concat(hourly_dataframes)\n",
    "    daily_data = pd.concat(daily_dataframes)\n",
    "    \n",
    "    return hourly_data, daily_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f7cd53a6",
   "metadata": {},
   "outputs": [],
   "source": [
    "hourly_weather_data, daily_weather_data = load_and_clean_weather_data()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "48216557",
   "metadata": {},
   "outputs": [],
   "source": [
    "hourly_weather_data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "935261b7-ae23-427c-97ff-ea31aa4e44c9",
   "metadata": {},
   "outputs": [],
   "source": [
    "hourly_weather_data.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a7dcb502-d1d1-447d-aa68-11bff0dc53b6",
   "metadata": {},
   "outputs": [],
   "source": [
    "hourly_weather_data.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4cb386ed",
   "metadata": {},
   "outputs": [],
   "source": [
    "daily_weather_data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f090eb94-a5b0-4d93-bf82-a596d2521b47",
   "metadata": {},
   "outputs": [],
   "source": [
    "daily_weather_data.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8c074aa3-a5f2-4586-8748-411e1e6c11da",
   "metadata": {},
   "outputs": [],
   "source": [
    "daily_weather_data.describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dd101f11",
   "metadata": {},
   "source": [
    "## Part 2: Storing Cleaned Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f3529cf6",
   "metadata": {},
   "outputs": [],
   "source": [
    "engine = db.create_engine(DATABASE_URL)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d2bea0ff",
   "metadata": {},
   "outputs": [],
   "source": [
    "# if using SQL (as opposed to SQLAlchemy), define the commands \n",
    "# to create your 4 tables/dataframes\n",
    "HOURLY_WEATHER_SCHEMA = \"\"\"\n",
    "TODO\n",
    "\"\"\"\n",
    "\n",
    "DAILY_WEATHER_SCHEMA = \"\"\"\n",
    "TODO\n",
    "\"\"\"\n",
    "\n",
    "TAXI_TRIPS_SCHEMA = \"\"\"\n",
    "TODO\n",
    "\"\"\"\n",
    "\n",
    "UBER_TRIPS_SCHEMA = \"\"\"\n",
    "TODO\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5f41e54b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# create that required schema.sql file\n",
    "with open(DATABASE_SCHEMA_FILE, \"w\") as f:\n",
    "    f.write(HOURLY_WEATHER_SCHEMA)\n",
    "    f.write(DAILY_WEATHER_SCHEMA)\n",
    "    f.write(TAXI_TRIPS_SCHEMA)\n",
    "    f.write(UBER_TRIPS_SCHEMA)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "02eccdba",
   "metadata": {},
   "outputs": [],
   "source": [
    "# create the tables with the schema files\n",
    "with engine.connect() as connection:\n",
    "    pass"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c122964f",
   "metadata": {},
   "source": [
    "### Add Data to Database"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0e68a363",
   "metadata": {},
   "outputs": [],
   "source": [
    "def write_dataframes_to_table(table_to_df_dict):\n",
    "    raise NotImplemented()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "45d6c06c",
   "metadata": {},
   "outputs": [],
   "source": [
    "map_table_name_to_dataframe = {\n",
    "    \"taxi_trips\": taxi_data,\n",
    "    \"uber_trips\": uber_data,\n",
    "    \"hourly_weather\": hourly_data,\n",
    "    \"daily_weather\": daily_data,\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "74004f96",
   "metadata": {},
   "outputs": [],
   "source": [
    "write_dataframes_to_table(map_table_name_to_dataframe)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8cb6e33e",
   "metadata": {},
   "source": [
    "## Part 3: Understanding the Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6a849e92",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Helper function to write the queries to file\n",
    "def write_query_to_file(query, outfile):\n",
    "    raise NotImplementedError()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ee70a777",
   "metadata": {},
   "source": [
    "### Query 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "db871d3c",
   "metadata": {},
   "outputs": [],
   "source": [
    "QUERY_1_FILENAME = \"\"\n",
    "\n",
    "QUERY_1 = \"\"\"\n",
    "TODO\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c5275f3a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# execute query either via sqlalchemy\n",
    "with engine.connect() as con:\n",
    "    results = con.execute(db.text(QUERY_1)).fetchall()\n",
    "results\n",
    "\n",
    "# or via pandas\n",
    "pd.read_sql(QUERY_1, con=engine)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a2ef04df",
   "metadata": {},
   "outputs": [],
   "source": [
    "write_query_to_file(QUERY_1, QUERY_1_FILENAME)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a13ced42",
   "metadata": {},
   "source": [
    "## Part 4: Visualizing the Data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6d9eef42",
   "metadata": {},
   "source": [
    "### Visualization 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0de8394c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# use a more descriptive name for your function\n",
    "def plot_visual_1(dataframe):\n",
    "    figure, axes = plt.subplots(figsize=(20, 10))\n",
    "    \n",
    "    values = \"...\"  # use the dataframe to pull out values needed to plot\n",
    "    \n",
    "    # you may want to use matplotlib to plot your visualizations;\n",
    "    # there are also many other plot types (other \n",
    "    # than axes.plot) you can use\n",
    "    axes.plot(values, \"...\")\n",
    "    # there are other methods to use to label your axes, to style \n",
    "    # and set up axes labels, etc\n",
    "    axes.set_title(\"Some Descriptive Title\")\n",
    "    \n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "847ced2f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_data_for_visual_1():\n",
    "    # Query SQL database for the data needed.\n",
    "    # You can put the data queried into a pandas dataframe, if you wish\n",
    "    raise NotImplementedError()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3c63e845",
   "metadata": {},
   "outputs": [],
   "source": [
    "some_dataframe = get_data_for_visual_1()\n",
    "plot_visual_1(some_dataframe)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
